{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\RecSys\\total_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by='timestamp')\n",
    "\n",
    "# Разделяем данные на train и test выборки\n",
    "train_size = 0.7\n",
    "train, test = train_test_split(data, train_size=train_size, shuffle=False)\n",
    "\n",
    "# Выводим размеры выборок\n",
    "print(f'Train size: {len(train)}')\n",
    "print(f'Test size: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем коллаборативную фильтрацию гибридным методом (как item- так и user- based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_filtration(interaction_matrix_scaled, user_similarity_df, item_similarity_df,  client, num_recommendations):\n",
    "    interacted_items = interaction_matrix_scaled.loc[client][interaction_matrix_scaled.loc[client] > 0].index.tolist()\n",
    "    # Функция для получения рекомендаций на основе User-based CF\n",
    "    def get_user_based_recommendations(user_id, num_recommendations=num_recommendations):\n",
    "        similar_users = user_similarity_df[user_id].sort_values(ascending=False).index[1:num_recommendations + 1]\n",
    "        recommended_items = interaction_matrix_scaled.loc[similar_users].sum().sort_values(ascending=False).index[:num_recommendations]\n",
    "        return recommended_items\n",
    "\n",
    "    # Функция для получения рекомендаций на основе Item-based CF\n",
    "    def get_item_based_recommendations(item_id, num_recommendations=num_recommendations*3):\n",
    "        similar_items = item_similarity_df[item_id].sort_values(ascending=False).index[1:num_recommendations + 1]\n",
    "        return similar_items\n",
    "\n",
    "    def get_all_item_recomendations(user_id, num_recommendations=num_recommendations):\n",
    "        item_based_recommendations = []\n",
    "        for item_id in interacted_items:\n",
    "            item_based_recommendations.extend(get_item_based_recommendations(item_id, num_recommendations * 3))\n",
    "        filtered_based_recommendations = [item for item in item_based_recommendations if item not in interacted_items]\n",
    "        recommendation_counts = Counter(filtered_based_recommendations)\n",
    "        # Получаем топ num_recommendations\n",
    "        top_recommendations = recommendation_counts.most_common(num_recommendations)\n",
    "        # Возвращаем только предметы\n",
    "        return [item for item, count in top_recommendations]\n",
    "\n",
    "\n",
    "    # Гибридная рекомендация\n",
    "    def hybrid_recommendation(user_id, num_recommendations=num_recommendations):\n",
    "        user_based_recommendations = get_user_based_recommendations(user_id, num_recommendations)\n",
    "        user_filtered_based_recommendations = [item for item in user_based_recommendations if item not in interacted_items]\n",
    "        item_based_recommendations = get_all_item_recomendations(user_id, num_recommendations)\n",
    "        # Объединение рекомендаций\n",
    "        # print(user_based_recommendations)\n",
    "        # print(item_based_recommendations)\n",
    "        combined_recommendations = set(user_based_recommendations).union(set(item_based_recommendations))\n",
    "        return tuple([user_filtered_based_recommendations, item_based_recommendations])\n",
    "\n",
    "    # Пример использования\n",
    "    return  hybrid_recommendation(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def get_filter_recs(data):\n",
    "    \n",
    "    # Создание матрицы взаимодействий\n",
    "    interaction_matrix = data.pivot(index='user_id', columns='item_id', values='timestamp').fillna(0)\n",
    "\n",
    "    # Нормализация матрицы взаимодействий\n",
    "    # scaler = StandardScaler()\n",
    "    # interaction_matrix_scaled = scaler.fit_transform(interaction_matrix)\n",
    "\n",
    "    # Коллаборативная фильтрация (User-based)\n",
    "    user_similarity = cosine_similarity(interaction_matrix)\n",
    "    user_similarity_df = pd.DataFrame(user_similarity, index=interaction_matrix.index, columns=interaction_matrix.index)\n",
    "\n",
    "    # Коллаборативная фильтрация (Item-based)\n",
    "    item_similarity = cosine_similarity(interaction_matrix.T)\n",
    "    item_similarity_df = pd.DataFrame(item_similarity, index=interaction_matrix.columns, columns=interaction_matrix.columns)\n",
    "    recommendation_dict ={}\n",
    "    for user in tqdm(data.user_id.unique()):\n",
    "        recommendation_dict[user] = hybrid_filtration(interaction_matrix, user_similarity_df, item_similarity_df,  user, num_recommendations = 50 )\n",
    "    return recommendation_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили словарь рекомендаций (как item- так и user- based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [12:24<00:00,  8.11it/s]\n"
     ]
    }
   ],
   "source": [
    "recommendation_dict = get_filter_recs(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим train-датасет для обучения бустинга - выберем пары, по которым не было взаимодействий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users = data['user_id'].unique()\n",
    "unique_items = data['item_id'].unique()\n",
    "\n",
    "# Создаем все возможные пары user_id и item_id\n",
    "all_pairs = pd.MultiIndex.from_product([unique_users, unique_items], names=['user_id', 'item_id']).to_frame(index=False)\n",
    "\n",
    "# Находим пары, которые есть в train\n",
    "train_pairs = train[['user_id', 'item_id']]\n",
    "\n",
    "# Удаляем пары, которые есть в train из all_pairs\n",
    "val = all_pairs.merge(train_pairs, on=['user_id', 'item_id'], how='left', indicator=True)\n",
    "val = val[val['_merge'] == 'left_only'].drop(columns=['_merge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_recommendation_columns(data, recommendation_dict):\n",
    "    \"\"\"Add columns indicating if there are recommendations for each algorithm.\"\"\"\n",
    "    recommended_algo1 = []\n",
    "    recommended_algo2 = []\n",
    "    for index, row in tqdm(data.iterrows()):\n",
    "        user = row['user_id']\n",
    "        item = row['item_id']\n",
    "        if user in recommendation_dict.keys():\n",
    "            if item in recommendation_dict[user][0]:\n",
    "                recommended_algo1.append(1)\n",
    "            else:\n",
    "                recommended_algo1.append(0)\n",
    "            if item in recommendation_dict[user][1]:\n",
    "                recommended_algo2.append(1)\n",
    "            else:\n",
    "                recommended_algo2.append(0)\n",
    "        else:\n",
    "            recommended_algo1.append(0)\n",
    "            recommended_algo2.append(0)\n",
    "    \n",
    "    return recommended_algo1, recommended_algo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим рекомендации из коллаборативной фильтрации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21661696it [25:30, 14152.84it/s]\n"
     ]
    }
   ],
   "source": [
    "val['recommended_algo1'], val['recommended_algo2'] = add_recommendation_columns(val, recommendation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_dict = test.set_index(['user_id', 'item_id'])['rating'].to_dict()\n",
    "\n",
    "# Функция для определения значения y\n",
    "def get_y(row):\n",
    "    user_id = row['user_id']\n",
    "    item_id = row['item_id']\n",
    "    \n",
    "    # Проверяем, есть ли пара в rating_dict\n",
    "    if (user_id, item_id) in rating_dict:\n",
    "        rating = rating_dict[(user_id, item_id)]\n",
    "        if rating <= 2:\n",
    "            return 0\n",
    "        elif rating == 3:\n",
    "            return 0.5\n",
    "        elif rating >= 4:\n",
    "            return 1\n",
    "    return 0.25  # Если пары нет в test_pairs\n",
    "\n",
    "# Применяем функцию к каждой строке DataFrame val\n",
    "val['y'] = val.apply(get_y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_random_share(df, share):\n",
    "    \"\"\"\n",
    "    Удаляет случайную долю строк из DataFrame, где y == 0.25.\n",
    "\n",
    "    :param df: DataFrame, из которого нужно удалить строки\n",
    "    :param share: Доля строк для удаления (например, 0.1 для 10%)\n",
    "    :return: Очищенный DataFrame\n",
    "    \"\"\"\n",
    "    # Фильтруем строки, где y == 0.25\n",
    "    rows_to_remove = df.query('y ==0.25 and recommended_algo1 ==0 and recommended_algo2 ==0')\n",
    "    \n",
    "    # Определяем количество строк для удаления\n",
    "    num_rows_to_remove = int(len(rows_to_remove) * share)\n",
    "    \n",
    "    # Проверяем, что количество строк для удаления не превышает доступное количество\n",
    "    if num_rows_to_remove > 0:\n",
    "        # Случайным образом выбираем строки для удаления\n",
    "        rows_to_remove_indices = rows_to_remove.sample(n=num_rows_to_remove, random_state=42).index\n",
    "        \n",
    "        # Удаляем выбранные строки из оригинального DataFrame\n",
    "        df_cleaned = df.drop(index=rows_to_remove_indices)\n",
    "    else:\n",
    "        # Если нет строк для удаления, возвращаем оригинальный DataFrame\n",
    "        df_cleaned = df.copy()\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам не хватает оперативки - придётся обучить catboost  на обрезанном датасете. Мы оставили все пары, которые есть в test, и по котрым есть рекомендации из коллаборативной фильтрации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_small = remove_random_share(val, 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2716381"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "len(val_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим фичи по клиента и itemам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_3860\\1232126811.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  client_features = data.groupby(by = 'user_id')['age_group', 'user_rating_count', 'user_genre0_share', 'user_genre1_share',\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_3860\\1232126811.py:11: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  'gender', 'age'].mean().reset_index()\n"
     ]
    }
   ],
   "source": [
    "client_features = data.groupby(by = 'user_id')['age_group', 'user_rating_count', 'user_genre0_share', 'user_genre1_share',\n",
    "    'user_genre2_share', 'user_genre3_share', 'user_genre4_share',\n",
    "    'user_genre5_share', 'user_genre6_share', 'user_genre7_share',\n",
    "    'user_genre8_share', 'user_genre9_share', 'user_genre10_share',\n",
    "    'user_genre11_share', 'user_genre12_share', 'user_genre13_share',\n",
    "    'user_genre14_share', 'user_genre15_share', 'user_genre16_share',\n",
    "    'user_genre17_share', 'user_timestamp_q0', 'user_timestamp_q10',\n",
    "    'user_timestamp_q25', 'user_timestamp_q33', 'user_timestamp_q50',\n",
    "    'user_timestamp_q67', 'user_timestamp_q75', 'user_timestamp_q90',\n",
    "    'user_timestamp_q100', 'user_timestamp_range', 'user_timestamp_iqr',\n",
    "    'gender', 'age'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_3860\\3081234336.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  item_features = data.groupby(by = 'item_id')['female_ratio', 'male_ratio',\n"
     ]
    }
   ],
   "source": [
    "item_features = data.groupby(by = 'item_id')['female_ratio', 'male_ratio',\n",
    "    'young_ratio', 'adult_ratio', 'senior_ratio',\n",
    "    'female_ratio_genre', 'male_ratio_genre', 'young_ratio_genre',\n",
    "    'adult_ratio_genre', 'senior_ratio_genre', 'item_rating_count',\n",
    "    'avg_rating_time_x', 'rating_time_range_x', \n",
    "    'genre_0', 'genre_1', 'genre_2', 'genre_3', 'genre_4', 'genre_5',\n",
    "    'genre_6', 'genre_7', 'genre_8', 'genre_9', 'genre_10', 'genre_11',\n",
    "    'genre_12', 'genre_13', 'genre_14', 'genre_15', 'genre_16', 'genre_17'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_with_client_features = val_small.merge(client_features, on='user_id', how='left')\n",
    "\n",
    "# Выполняем left join с item_features по item_id\n",
    "val_final = val_with_client_features.merge(item_features, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>recommended_algo1</th>\n",
       "      <th>recommended_algo2</th>\n",
       "      <th>y</th>\n",
       "      <th>user_rating_count</th>\n",
       "      <th>user_genre0_share</th>\n",
       "      <th>user_genre1_share</th>\n",
       "      <th>user_genre2_share</th>\n",
       "      <th>user_genre3_share</th>\n",
       "      <th>...</th>\n",
       "      <th>genre_8</th>\n",
       "      <th>genre_9</th>\n",
       "      <th>genre_10</th>\n",
       "      <th>genre_11</th>\n",
       "      <th>genre_12</th>\n",
       "      <th>genre_13</th>\n",
       "      <th>genre_14</th>\n",
       "      <th>genre_15</th>\n",
       "      <th>genre_16</th>\n",
       "      <th>genre_17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1069</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>287.0</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.2125</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.1498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3051</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>287.0</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.2125</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.1498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  recommended_algo1  recommended_algo2     y  \\\n",
       "0        0     1069                  0                  0  0.25   \n",
       "1        0     3051                  0                  0  1.00   \n",
       "\n",
       "   user_rating_count  user_genre0_share  user_genre1_share  user_genre2_share  \\\n",
       "0              287.0              0.331             0.2125             0.0906   \n",
       "1              287.0              0.331             0.2125             0.0906   \n",
       "\n",
       "   user_genre3_share  ...  genre_8  genre_9  genre_10  genre_11  genre_12  \\\n",
       "0             0.1498  ...      0.0      0.0       0.0       1.0       0.0   \n",
       "1             0.1498  ...      0.0      0.0       0.0       0.0       0.0   \n",
       "\n",
       "   genre_13  genre_14  genre_15  genre_16  genre_17  \n",
       "0       1.0       0.0       0.0       0.0       0.0  \n",
       "1       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[2 rows x 67 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'item_id', 'recommended_algo1', 'recommended_algo2', 'y',\n",
       "       'user_rating_count', 'user_genre0_share', 'user_genre1_share',\n",
       "       'user_genre2_share', 'user_genre3_share', 'user_genre4_share',\n",
       "       'user_genre5_share', 'user_genre6_share', 'user_genre7_share',\n",
       "       'user_genre8_share', 'user_genre9_share', 'user_genre10_share',\n",
       "       'user_genre11_share', 'user_genre12_share', 'user_genre13_share',\n",
       "       'user_genre14_share', 'user_genre15_share', 'user_genre16_share',\n",
       "       'user_genre17_share', 'user_timestamp_q0', 'user_timestamp_q10',\n",
       "       'user_timestamp_q25', 'user_timestamp_q33', 'user_timestamp_q50',\n",
       "       'user_timestamp_q67', 'user_timestamp_q75', 'user_timestamp_q90',\n",
       "       'user_timestamp_q100', 'user_timestamp_range', 'user_timestamp_iqr',\n",
       "       'age', 'female_ratio', 'male_ratio', 'young_ratio', 'adult_ratio',\n",
       "       'senior_ratio', 'female_ratio_genre', 'male_ratio_genre',\n",
       "       'young_ratio_genre', 'adult_ratio_genre', 'senior_ratio_genre',\n",
       "       'item_rating_count', 'avg_rating_time_x', 'rating_time_range_x',\n",
       "       'genre_0', 'genre_1', 'genre_2', 'genre_3', 'genre_4', 'genre_5',\n",
       "       'genre_6', 'genre_7', 'genre_8', 'genre_9', 'genre_10', 'genre_11',\n",
       "       'genre_12', 'genre_13', 'genre_14', 'genre_15', 'genre_16', 'genre_17'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим бустинг (в этом ноутбуке он упал, мало памяти - будет доп файл, где будет видно, что модель обучена успешно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.1645482\ttotal: 2.24s\tremaining: 37m 17s\n",
      "100:\tlearn: 0.1357963\ttotal: 1m 29s\tremaining: 13m 17s\n",
      "200:\tlearn: 0.1335464\ttotal: 2m 52s\tremaining: 11m 26s\n",
      "300:\tlearn: 0.1320875\ttotal: 4m 12s\tremaining: 9m 46s\n",
      "400:\tlearn: 0.1310442\ttotal: 5m 41s\tremaining: 8m 30s\n",
      "500:\tlearn: 0.1302476\ttotal: 7m 32s\tremaining: 7m 31s\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "bad allocation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 12\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m CatBoostRegressor(iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,  \u001b[38;5;66;03m# Количество итераций\u001b[39;00m\n\u001b[0;32m      6\u001b[0m                           learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,  \u001b[38;5;66;03m# Скорость обучения\u001b[39;00m\n\u001b[0;32m      7\u001b[0m                           depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,  \u001b[38;5;66;03m# Глубина деревьев\u001b[39;00m\n\u001b[0;32m      8\u001b[0m                           verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      9\u001b[0m                           ignored_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Вывод информации каждые 100 итераций\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Обучение модели\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X, y)\n",
      "File \u001b[1;32mc:\\Users\\asus\\Anaconda3\\Lib\\site-packages\\catboost\\core.py:5734\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5732\u001b[0m     CatBoostRegressor\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5734\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, cat_features, text_features, embedding_features, \u001b[38;5;28;01mNone\u001b[39;00m, sample_weight, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, baseline,\n\u001b[0;32m   5735\u001b[0m                  use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n\u001b[0;32m   5736\u001b[0m                  verbose_eval, metric_period, silent, early_stopping_rounds,\n\u001b[0;32m   5737\u001b[0m                  save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "File \u001b[1;32mc:\\Users\\asus\\Anaconda3\\Lib\\site-packages\\catboost\\core.py:2357\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2353\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2355\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[0;32m   2356\u001b[0m     plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(\n\u001b[0;32m   2358\u001b[0m         train_pool,\n\u001b[0;32m   2359\u001b[0m         train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_sets\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   2360\u001b[0m         params,\n\u001b[0;32m   2361\u001b[0m         allow_clear_pool,\n\u001b[0;32m   2362\u001b[0m         train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2363\u001b[0m     )\n\u001b[0;32m   2365\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2366\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32mc:\\Users\\asus\\Anaconda3\\Lib\\site-packages\\catboost\\core.py:1761\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1761\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[38;5;241m.\u001b[39m_object \u001b[38;5;28;01mif\u001b[39;00m init_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:4624\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:4673\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCatBoostError\u001b[0m: bad allocation"
     ]
    }
   ],
   "source": [
    "X = val_final.drop(columns=['y'])  # Все столбцы, кроме 'y'\n",
    "y = val_final['y']  # Целевая переменная\n",
    "\n",
    "# Создаем и обучаем модель\n",
    "model = CatBoostRegressor(iterations=1000,  # Количество итераций\n",
    "                          learning_rate=0.1,  # Скорость обучения\n",
    "                          depth=6,  # Глубина деревьев\n",
    "                          verbose=100,\n",
    "                          ignored_features = ['user_id', 'item_id'])  # Вывод информации каждые 100 итераций\n",
    "\n",
    "# Обучение модели\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, 'catboost.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
